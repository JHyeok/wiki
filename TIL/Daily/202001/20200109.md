### 20200109
---

**.NET에서 Kafka를 사용하면서 배운 내용들**

- Kafka를 이용함으로써 Consumer가 장애가 발생하여도 Kafka에는 message(데이터)가 쌓이기 때문에 유실되지 않음
  - 원하는 시점에 Consumer를 재시작해도 문제가 발생하지 않기 때문에 수정/배포에 자유로움.
- 수집된 데이터를 실시간으로 Kafka에 전송
  - 수집된 데이터는 Binary JSON으로 Serialize 해서 전송해야 한다. 
- Kafka의 Topic을 지정해서 message를 분류
- Kafka에 전달되는 message의 크기가 크니까 오류가 발생했다.
  - Kafka의 기본 message의 크기는 최대 1Mb이다.
  - Kafka의 message.max.bytes를 이용해 오류 해결할 수 있다.
-	Kafka Consumer에서는 데이터를 Consume 할 때, Binary JSON을 Deserialize 해야 한다.
- Kafka의 데이터를 Product 하고 Consume 하는데 데이터가 유실되는 경우의 해결책이 필요하다.
  - Entity Framework에서 재시도를 통해서 오류가 나지 않도록 먼저 조치한다.
  - Amazon S3를 이용해서 데이터 유실 방지를 할 수 있다.
-	Kafka의 message를 Consume 할 때, 속도가 느린 경우가 있다는 것을 확인
  - message의 크기가 크면 속도가 느리다.
  - 포럼에서 Kafka 전문가가 Kafka는 큰 message를 처리하는 데는 적절하지 않다는 답변을 확인했다.
  - Kafka에 message를 Produce 할 때, message를 압축하는게 좋다.
  - message의 크기는 줄었고, 속도 문제에서 기존보다 좋아졌다.
- Kafka의 Parition을 늘려서 속도를 더 빠르게 할 수 있다.
  - Partion을 늘리면 속도가 늘어나는 것은 맞지만 Kafka에 들어오는 message의 순서를 보장하지 못한다.
  - Kafka의 Partion이 늘어났을 때 Consumer도 늘어나야 하기 때문에 Partion을 늘리는데 부담이 생긴다.
  - Partion을 늘리지 않고도 messag의 크기를 줄인 것만으로도 큰 속도 향상이 있었기 때문에 보류했다.
