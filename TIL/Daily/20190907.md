### 20190907
---

#### 머신러닝

- 목적에 맞는 데이터를 투입하는게 중요하다
- 머신러닝 모델 성능은 데이터

ex) 알파고 - 바둑: good / 장기: X\
환경: 바둑 (목적:바둑을 잘두자!)\ 
데이터: 바둑 기록

인공지능 > 머신러닝 > 딥러닝

무슨일이 일어났는가?(기술적 분석) / 왜 일어났는가?(진단적 분석) = 통계학, 비지도학습\
무슨일이 일어날 것인가?(예측 분석) = 일반적인 머신러닝의 범위, 지도학습

ex) 교통사고 분석
1. 기술적 분석: 오늘 교통사고 200건 발생!
2. 진단적 분석: 비 많이 옴
3. 예측 분석: 내일 비가 많이 오면 교통사고 200건이 일어날 것이다
4. 처방적 분석: 교통사고 200건 예측 됬으니까 도로에 미끄럼 방지를 하겠다.

군집 = 쉽게 생각하면 그룹\
군집화 = grouping(그룹으로 묶는 작업)\
군집에 라벨을 달아주는 작업은 사람이 해야하는 분야이다.

ex) 거래 데이터(영수증 데이터) --> 기저귀를 사면 맥주를 사더라, 술을 사면 담배를 사더라 ... 수만가지의 규칙 (연관규칙)\
어느 규칙이 유효할지는 사람이 하기 때문에 위의 예제는 비지도 규칙이다.

크롤링: 웹문서 수집\
정밀도: 긍정이라고 예측한 것중에서 실제 긍정 비율\
재현율: 실제 긍정 가운데 제대로 예측된 긍정 비율\
과적합 => 학습데이터에 치우쳐서 일반화 능력이 떨어지는 것\
sklearn: 머신러닝 패키지

분류기, 예측모델 -> (c.f) 나이브베이즈 : 분류o, 예측x

**데이터 관련 용어**\
클래스 = 라벨 = 종속 변수 = 출력 변수\
특징 = 속성 = 예측변수 = 입력변수 = 독립변수\
레코드 = 샘플

**일반 거리 척도**\
Euclidean : 가장 많이 쓰임, 직선거리, 특징들이 연속형일 때 많이 쓰임\
Manhattan : 특징들이 정수형일 때 많이 쓰임\
**이진 데이터 거리 척도**\
Matching : 전부 이진형일때 쓰임

**자카드 유사도**\
ex) 대한민국 국민 -> 특징:페라리 스포츠카 보유여부\
갖고 있는 두 사람이 갖고 있지 않은 두 사람보다 유사할 가능성이 크다.\
둘 다 0으로 유사한 것은 의미가 없다.\
최소한 하나가 1인것만 유사도 계산하는데 사용\
대부분 0을 가지는 데이터(=희소한 데이터)의 경우에 많이 쓰임 => 텍스트 분류, 추천시스템 개발

1 = x1: 1 / x2: 0 / x3: 1 / x4: 0\
2 = x1: 1 / x2: 1 / x3: 0 / x4: 0\
1~2의 유사도 2/4\
1~2의 자카드 유사도 1/3

**의사결정나무**\
Tree학습 기준 => 불순도\
장점 : 설명력(=예측하는 과정을 설명할 수 있는 능력)\
white box model\
예측 근거가 필요한 분야에 많이 쓰임(ex.대출연쳬여부)\
학습 속도가 빠른편이다. (질문이 많더라도)\
사전 분류치기(시간 빠름)는 거의 쓰지 않음, 사후 분류치기(성능 좋음)를 많이 씀\
어차피 시간이 빠르기 때문에 성능 좋은 사후 분류치기를 많이 쓴다고 함

**서포트 벡터 머신**\
마진(여유공간)을 최대화 할 수 있는 w, b를 추정하는 모델\
제약: 분류정확도가 보장 필요\
최적화모델 = 목적식 + 제약식 => 제약식을 만족하는 해 가운데, 목적식을 최대화/최소화 하는 해를 찾기위한 수리 모델\
이론적으로, 일반화 능력이 가장 우수한 모델

하드마진SVM => 문제: 제약을 만족하는 경우가 거의 없다.(사실상 불가능)\
--> 소프트마진SVM => 문제: 대부분의 데이터가 선형분리가 안되기 때문에 감수해야하는 오차가 매우 큼\
--> 커널트릭 사용: 커널함수

과제아닌 과제 Polynomial kernel\
Polynomial kernel는 시간이 너무 오래 걸리기 때문에 보통 실무에서 사용하지 않음.

**서포트 벡터 회귀**\
데이터양에 영향을 너무 많이 받음(데이터가 매우 커야 성능이 우수함)\
오차의 합 최소화, 마진 최대화

**신경망**\
인간의 뇌를 모방한 모델\
은닉층의 개수가 3개 이상이면, 기본신경망 / 3개 이하면, 심층신경망\
데이터가 적으면 성능이 낮다.\
입력 노드의 개수 = 입력 변수의 개수\
일반적으로 출력 노드의 개수 = 출력 변수의 개수 = 1\
출력변수가 가지는 값의 개수를 변경해주는 작업이 원한인코딩이라고 한다\
최종적인 값을 계산하는게 소프트맥스 함수이다.

모델이 복잡 -> 과적합 가능성이 높다 -> 데이터가 매우 많이 필요\
if) 데이터가 적다 -> 성능이 나쁠 수 밖에 없다.

**신경망 활성화 함수**\
층이 많아지면 ReLu함수가 표준으로 쓰임\
Gradient Vanishing: 학습이 이루어지지 않음

**내리막 경사법**
어떤 일반적인 모델의 경우에는 오차제곱합을 최소화 하는게 목적인데, 일반적으로 연립방정식을 풀어서 세타를 찾는데 동시에 만족하는건 거의 불가능, 정확하게 풀수는 없고, 분석적으로 풀이는 불가능하고 그럴듯한 방식을 찾아본게 내리막 경사법\
학습률이 크면 진행폭이 크기 때문에 장점은 빠르다, 단점은 수렴못할가능성, 학습률이 작으면 장점은 수령가능성이 높고, 단점은 시간이 오래걸린다.\
내리막 경사법을 기본으로 하는 것이 오류역전파 알고리즘이라고 부른다. 예측하고 업데이트하고, 예측하고 업데이트한다고 해서 오류역전파라고 부름.

---
#### 참고

패스트캠퍼스 2주 안에 Data Scientist되기