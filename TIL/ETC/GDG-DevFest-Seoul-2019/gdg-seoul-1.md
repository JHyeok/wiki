# 쉽게 따라할 수 있는 임베딩 활용 (이기창, NAVER)

임베딩: 단어나 문장을 벡터로 바꾼 것 혹은 그 과정\
자연어처리에서 임베딩은 벡터 내지 벡터화\
빈도 그 자체를 임베딩으로 씀

임베딩으로 할 수 있는 것들
- 관련도/유사도 계산
- 시각화
- 벡터 연산(유추 평가): 아들-딸+소녀 = 소년
- 전이학습(transfer learning): 임베딩한 것들 다른 딥러닝 모델의 입력값으로 사용

#### 단어 수준 임베딩 (워드 임베딩)

사람같은 경우 새로운 책을 읽을 때, 완전히 학습하는 것이 아님 배경지식을 활용해서 독해를 하게됨\
이 딥러닝 모델도 랜덤같은 경우 학습을 완전히 제로, FastText 는 이미 공개된 의미나 문법 정보를 한번 보고나서 하게 됨.

단어 임베딩 구축 - fastText.git

각각의 단어 임베딩 기법들이 조금씩 차이가 있지만 0.70~0.73 사이

그거 단어벡터를 cos 한건데 우리가 원하는 task의 성능을 70%이상을 달성.

시사점
복잡한 딥러닝 모델을 써도 80%대 성능을 기록\
단어 임베딩 품질이 좋으면 자연어 처리의 성능을 높일 수 있음\
임베딩 만으로도 저 성능이면 임베딩 자체에 집중을 할만하다.

임베딩이 어떻게 의미를 가지는가
- 일단 빈도를 센다. -> 문서를 쓴 이의 의도는 단어 사용 패턴에 드러난다.
- 단어가 어떤 순서로 나타나는지 살핀다. (시퀀스 정보에 의미가 녹아 있다) ELMo, BERT
- 단어가 어떤 단어와 주로 같이 나타나는지 살핀다.(문맥에 의미가 녹아 있다) word2vec, fastText,Glove ... 워드임베딩


#### 문장 수준 임베딩

최근 ELMo, BERT 등 다양한 문장 수준 임베딩 등장 (성능 BERT > ELMo)

문장 수준 임베딩의 장점은 동음이의어 분간 가능, 다시 말해 문장의 문맥적 의미를 벡터화할 수 있음

우리들이 만든 말뭉치를 넣으면 커스텀해서 사용 가능함, ELMo는 87% 나왔음

BERT-Korean-Model.git 공개 되어 있음.

전이학습 듀토리얼: ratsgo/embedding.git
